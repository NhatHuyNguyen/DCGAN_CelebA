{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import time\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.train import Checkpoint, CheckpointManager\r\n",
    "from tensorflow.data import Dataset\r\n",
    "from tensorflow.data.experimental import AUTOTUNE\r\n",
    "from tensorflow.keras.models import Model, Sequential\r\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\r\n",
    "from tensorflow.keras.metrics import Mean\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "from tensorflow.keras.initializers import TruncatedNormal, RandomNormal\r\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, BatchNormalization, Conv2D, Conv2DTranspose, \\\r\n",
    "        LeakyReLU, Flatten, SpatialDropout2D, Dropout, MaxPool2D, GlobalAvgPool2D, Concatenate, LayerNormalization\r\n",
    "\r\n",
    "from IPython import display\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BASE_PATH = 'C:/Users/s4571730/Downloads/img_align_celeba'\r\n",
    "RANDOM_STATE = 7\r\n",
    "SHUFFLE_BUFFER = 32_000\r\n",
    "IMAGE_SIZE = (178, 218)\r\n",
    "BATCH_SIZE = 128\r\n",
    "GEN_NOISE_SHAPE = (6, 5, 8)\r\n",
    "PREDICT_COUNT = 9\r\n",
    "GEN_LR = 4e-6\r\n",
    "GEN_BETA_1 = 0.5\r\n",
    "DISC_LR = 1e-6\r\n",
    "DISC_BETA_1 = 0.9\r\n",
    "GEN_RELU_ALPHA = 0.2\r\n",
    "DISC_RELU_ALPHA = 0.3\r\n",
    "EPOCHS = 50\r\n",
    "DISC_LABEL_SMOOTHING = 0.25\r\n",
    "PLOTS_DPI = 150\r\n",
    "RETRAIN = os.path.isfile('./ckpt/checkpoint')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "image_names = Dataset.list_files(os.path.join(BASE_PATH, '*.jpg'), seed = RANDOM_STATE)\r\n",
    "image_count = image_names.cardinality().numpy()\r\n",
    "print(f\"\\nTotal number of image files: {image_count}\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load_image_data(filename):\r\n",
    "    img = tf.io.read_file(filename)\r\n",
    "    img = tf.io.decode_jpeg(img, channels = 3)\r\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\r\n",
    "    return (img - 127.5)/127.5\r\n",
    "\r\n",
    "train_ds = image_names.cache() \\\r\n",
    "        .shuffle(SHUFFLE_BUFFER) \\\r\n",
    "        .map(load_image_data, num_parallel_calls = AUTOTUNE) \\\r\n",
    "        .batch(BATCH_SIZE, drop_remainder = True) \\\r\n",
    "        .prefetch(buffer_size = AUTOTUNE)\r\n",
    "\r\n",
    "train_ds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(nrows = 3, ncols = 3, figsize = (9, 11))\r\n",
    "\r\n",
    "sample_images = [i for i in train_ds.take(1)][0].numpy()\r\n",
    "\r\n",
    "for i, ax in enumerate(axes.flatten()):\r\n",
    "    ax.imshow((sample_images[i] * 0.5) + 0.5)\r\n",
    "    ax.axis(False)\r\n",
    "    ax.grid(False)\r\n",
    "\r\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generator_model():\r\n",
    "    weight_init = TruncatedNormal(mean = 0.0, stddev = 0.02)\r\n",
    "    model = Sequential(name='Generator')\r\n",
    "    model.add(Flatten(input_shape=GEN_NOISE_SHAPE))\r\n",
    "    model.add(Dense(6 * 5 * 512, use_bias = False, kernel_initializer = weight_init, \r\n",
    "                activation = LeakyReLU(GEN_RELU_ALPHA), name = 'Gen_Dense'))\r\n",
    "    model.add(Reshape((6, 5, 512), name = 'Gen_Reshape'))\r\n",
    "    model.add(SpatialDropout2D(0.2, name = 'Gen_SD_1'))\r\n",
    "    model.add(Conv2DTranspose(512, (3, 3), padding='same', activation=LeakyReLU(GEN_RELU_ALPHA), use_bias = False,\r\n",
    "                               kernel_initializer=weight_init, name='Gen_Conv_T_1'))\r\n",
    "    model.add(Conv2DTranspose(256, (3, 3), padding = 'same', strides = (2, 2), use_bias = False,\r\n",
    "                               kernel_initializer = weight_init, name = 'Gen_Conv_T_2'))\r\n",
    "    model.add(BatchNormalization(name = 'Gen_BN_1'))\r\n",
    "    model.add(LeakyReLU(GEN_RELU_ALPHA, name = 'Gen_LR_1'))\r\n",
    "    model.add(Conv2DTranspose(128, (4, 4), padding = 'same', activation = LeakyReLU(GEN_RELU_ALPHA), use_bias = False, \r\n",
    "                               kernel_initializer = weight_init, name='Gen_Conv_T_3'))\r\n",
    "    model.add(Conv2DTranspose(64, (4, 4), padding = 'same', strides = (2, 2), use_bias = False, \r\n",
    "                               kernel_initializer = weight_init, name = 'Gen_Conv_T_4'))\r\n",
    "    model.add(BatchNormalization(name = 'Gen_BN_2'))\r\n",
    "    model.add(LeakyReLU(GEN_RELU_ALPHA, name = 'Gen_LR_2'))\r\n",
    "    model.add(SpatialDropout2D(0.15, name = 'Gen_SD_3'))\r\n",
    "    model.add(Conv2DTranspose(8, (6, 6), padding = 'same', strides = (2, 2), use_bias = False, \r\n",
    "                               kernel_initializer = weight_init, name = 'Gen_Conv_T_6'))\r\n",
    "    model.add(BatchNormalization(name = 'Gen_BN_4'))\r\n",
    "    model.add(LeakyReLU(GEN_RELU_ALPHA, name = 'Gen_LR_4'))\r\n",
    "    model.add(SpatialDropout2D(0.15, name = 'Gen_SD_4'))\r\n",
    "    model.add(Conv2DTranspose(8, (7, 7), padding = 'same', activation = LeakyReLU(GEN_RELU_ALPHA), use_bias = False,\r\n",
    "                               kernel_initializer = weight_init, strides = (2, 2), name = 'Gen_Conv_T_7'))\r\n",
    "    model.add(Conv2DTranspose(3, (5, 5), padding = 'same', kernel_initializer = weight_init, use_bias = False,\r\n",
    "                               activation = 'tanh', name = 'Gen_Conv_T_8'))\r\n",
    "    return model\r\n",
    "    \r\n",
    "generator = generator_model()\r\n",
    "generator.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def discriminator_model():\r\n",
    "    input_layer = Input(shape = (*IMAGE_SIZE, 3), name = 'Disc_Input')\r\n",
    "    \r\n",
    "    conv_1 = Conv2D(32, (4, 4), activation = LeakyReLU(DISC_RELU_ALPHA), padding = 'same', name = 'Disc_Conv_1')(input_layer)\r\n",
    "    max_pool_1 = MaxPool2D(2, name = 'Disc_MP_1')(conv_1)\r\n",
    "    conv_2 = Conv2D(64, (4, 4), activation = LeakyReLU(DISC_RELU_ALPHA), padding = 'same', name = 'Disc_Conv_2')(max_pool_1)\r\n",
    "    max_pool_2 = MaxPool2D(2, name = 'Disc_MP_2')(conv_2)\r\n",
    "    global_pool_1 = GlobalAvgPool2D(name = 'Disc_GAP_1')(max_pool_2)\r\n",
    "    \r\n",
    "    sp_dropout_1 = SpatialDropout2D(0.2, name = 'Disc_SD_1')(max_pool_2)\r\n",
    "    conv_3 = Conv2D(128, (3, 3), activation = LeakyReLU(DISC_RELU_ALPHA), padding = 'same', name = 'Disc_Conv_3')(sp_dropout_1)\r\n",
    "    max_pool_3 = MaxPool2D(2, name = 'Disc_MP_3')(conv_3)\r\n",
    "    conv_4 = Conv2D(256, (3, 3), activation = LeakyReLU(DISC_RELU_ALPHA), padding = 'same', name = 'Disc_Conv_4')(max_pool_3)\r\n",
    "    max_pool_4 = MaxPool2D(2, name = 'Disc_MP_4')(conv_4)\r\n",
    "    global_pool_2 = GlobalAvgPool2D(name = 'Disc_GAP_2')(max_pool_4)\r\n",
    "    \r\n",
    "    sp_dropout_2 = SpatialDropout2D(0.2, name = 'Disc_SD_2')(max_pool_4)\r\n",
    "    conv_5 = Conv2D(512, (2, 2), activation = LeakyReLU(DISC_RELU_ALPHA), padding = 'same', name = 'Disc_Conv_5')(sp_dropout_2)\r\n",
    "    max_pool_5 = MaxPool2D(2, name = 'Disc_MP_5')(conv_5)\r\n",
    "    global_pool_3 = GlobalAvgPool2D(name = 'Disc_GAP_3')(max_pool_5)\r\n",
    "    \r\n",
    "    concat = Concatenate(name = 'Disc_Concat')([global_pool_1, global_pool_2, global_pool_3])\r\n",
    "    dropout = Dropout(0.2, name = 'Disc_Dropout')(concat)\r\n",
    "    dense_1 = Dense(32, activation = LeakyReLU(DISC_RELU_ALPHA), name = 'Disc_Dense_1')(dropout)\r\n",
    "    dense_2 = Dense(1, name = 'Disc_Dense_2')(dense_1)\r\n",
    "    \r\n",
    "    return Model(inputs = input_layer, outputs = dense_2, name = 'Discriminator')\r\n",
    "    \r\n",
    "discriminator = discriminator_model()\r\n",
    "discriminator.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cross_entropy = BinaryCrossentropy(from_logits = True)\r\n",
    "gen_mean_loss = Mean(name = \"Generator mean loss\")\r\n",
    "disc_mean_loss = Mean(name = \"Discriminator mean loss\")\r\n",
    "generator_optimizer = Adam(GEN_LR, beta_1 = GEN_BETA_1)\r\n",
    "discriminator_optimizer = Adam(DISC_LR, beta_1 = DISC_BETA_1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "checkpoint_dir = './ckpt'\r\n",
    "\r\n",
    "checkpoint = Checkpoint(\r\n",
    "    step = tf.Variable(1),\r\n",
    "    generator_optimizer = generator_optimizer,\r\n",
    "    discriminator_optimizer = discriminator_optimizer,\r\n",
    "    generator = generator,\r\n",
    "    discriminator = discriminator)\r\n",
    "\r\n",
    "ckpt_manager = CheckpointManager(checkpoint, checkpoint_dir, max_to_keep = 5)\r\n",
    "\r\n",
    "EPOCH_START = 1\r\n",
    "if RETRAIN:\r\n",
    "    checkpoint.restore(ckpt_manager.latest_checkpoint)\r\n",
    "    EPOCH_START = checkpoint.step.numpy()\r\n",
    "\r\n",
    "print(f\"Starting training from Epoch {EPOCH_START}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tf.random.set_seed(RANDOM_STATE)\r\n",
    "seed_noise = tf.random.normal([PREDICT_COUNT, *GEN_NOISE_SHAPE], seed = RANDOM_STATE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_images(seed, save = False, epoch = None):\r\n",
    "    pred = generator(seed, training = False)\r\n",
    "\r\n",
    "    fig, axes = plt.subplots(nrows = 3, ncols = 3, figsize = (9, 11))\r\n",
    "\r\n",
    "    for i, ax in enumerate(axes.flatten()):\r\n",
    "        ax.imshow((pred[i] * 0.5) + 0.5)\r\n",
    "        ax.axis(False)\r\n",
    "        ax.grid(False)\r\n",
    "\r\n",
    "    plt.suptitle('Generator Predictions', fontsize = 20)\r\n",
    "    \r\n",
    "    plt.tight_layout()\r\n",
    "\r\n",
    "    if save:\r\n",
    "        plt.savefig(f'Pred_Epoch_{epoch:04d}.png', dpi = PLOTS_DPI, facecolor = 'white', \r\n",
    "                transparent = False, bbox_inches = 'tight')\r\n",
    "        plt.close()\r\n",
    "    \r\n",
    "generate_images(seed_noise)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def discriminator_loss(real_output, fake_output):\r\n",
    "    pos_labels = tf.ones_like(real_output) - (tf.random.uniform(real_output.shape) * DISC_LABEL_SMOOTHING)\r\n",
    "    neg_labels = tf.zeros_like(fake_output) + (tf.random.uniform(fake_output.shape) * DISC_LABEL_SMOOTHING)\r\n",
    "    real_loss = cross_entropy(pos_labels, real_output)\r\n",
    "    fake_loss = cross_entropy(neg_labels, fake_output)\r\n",
    "    total_loss = real_loss + fake_loss\r\n",
    "    return total_loss\r\n",
    "\r\n",
    "def generator_loss(fake_output):\r\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@tf.function\r\n",
    "def train_step(images):\r\n",
    "    noise = tf.random.normal([BATCH_SIZE, *GEN_NOISE_SHAPE])\r\n",
    "\r\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n",
    "        generated_images = generator(noise, training = True)\r\n",
    "\r\n",
    "        real_output = discriminator(images, training = True)\r\n",
    "        fake_output = discriminator(generated_images, training = True)\r\n",
    "\r\n",
    "        gen_loss = generator_loss(fake_output)\r\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\r\n",
    "\r\n",
    "    gen_mean_loss(gen_loss)\r\n",
    "    disc_mean_loss(disc_loss)\r\n",
    "\r\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\r\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\r\n",
    "\r\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\r\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gen_losses = []\r\n",
    "disc_losses = []\r\n",
    "\r\n",
    "def train(dataset, epochs):\r\n",
    "    for epoch in range(epochs):\r\n",
    "        start = time.time()\r\n",
    "\r\n",
    "        gen_mean_loss.reset_states()\r\n",
    "        disc_mean_loss.reset_states()\r\n",
    "        \r\n",
    "        print(f\"\\nTraining Epoch {epoch + EPOCH_START}\\n\")\r\n",
    "        \r\n",
    "        for batch_ind, image_batch in enumerate(dataset):\r\n",
    "            train_step(image_batch)\r\n",
    "\r\n",
    "            if (batch_ind + 1) % 10 == 0:\r\n",
    "                print(\". \", end = '')\r\n",
    "            if (batch_ind + 1) % 250 == 0:\r\n",
    "                print(f\"{batch_ind + 1}\")\r\n",
    "        \r\n",
    "        checkpoint.step.assign_add(1)\r\n",
    "\r\n",
    "        display.clear_output(wait = True)\r\n",
    "        \r\n",
    "        generate_images(seed_noise, True, epoch + EPOCH_START)\r\n",
    "\r\n",
    "        if (epoch + EPOCH_START) % 5 == 0:\r\n",
    "            ckpt_manager.save()\r\n",
    "            \r\n",
    "        gen_losses.append(gen_mean_loss.result())\r\n",
    "        disc_losses.append(disc_mean_loss.result())\r\n",
    "\r\n",
    "        print(f\"\\nEpoch: {epoch + EPOCH_START}\\n\")\r\n",
    "        print(f'Generator Loss: {gen_mean_loss.result():.4f}')\r\n",
    "        print(f'Discriminator Loss: {disc_mean_loss.result():.4f}')\r\n",
    "        print (f'Time elapsed: {time.time() - start:.2f} s')\r\n",
    "\r\n",
    "    display.clear_output(wait = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train(train_ds, EPOCHS)\r\n",
    "\r\n",
    "print(f'Final Generator Loss: {gen_mean_loss.result()}')\r\n",
    "print(f'Final Discriminator Loss: {disc_mean_loss.result()}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generate_images(seed_noise)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install tensorflow-datasets\r\n",
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "import tensorflow_datasets as tfds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "tfds.load('celeb_a', split='train')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\s4571730\\tensorflow_datasets\\celeb_a\\2.0.1...\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('s4571730': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "adb3a3fd769e812f48823833d444281c5b9ac11f1bf8927cc218069cceb30c32"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}